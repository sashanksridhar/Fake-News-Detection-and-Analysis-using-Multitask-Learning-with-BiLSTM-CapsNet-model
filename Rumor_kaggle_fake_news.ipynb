{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rumor_kaggle_fake_news.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHt02t8miN0D",
        "outputId": "7cc7c132-9beb-4376-e538-6034ff410a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip3 uninstall -y keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.4.3:\n",
            "  Successfully uninstalled Keras-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4zD-aigisVO",
        "outputId": "9da607d0-3905-4448-b942-94f3027df721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!pip3 install keras==2.1.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/89/58ee5f56a9c26957d97217db41780ebedca3154392cb903c3f8a08a52208/Keras-2.1.2-py2.py3-none-any.whl (304kB)\n",
            "\r\u001b[K     |█                               | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 24.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 18.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 286kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 296kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.4.1)\n",
            "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIRZkHojAiE",
        "outputId": "6f1a2ffb-bbc9-497c-9b00-ec3f206f32e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip3 uninstall -y tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Successfully uninstalled tensorflow-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1QbmxH6jhaC",
        "outputId": "71e42209-935b-4efb-a7fb-9c0556f5d292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.35.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.12.4)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.32.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (50.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.0)\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ck4Q2lj2G4",
        "outputId": "50aaf7b9-6740-4ab0-f5d6-a7329c176ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "#! -*- coding: utf-8 -*-\n",
        "# refer: https://kexue.fm/archives/5112\n",
        "\n",
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "\n",
        "#define our own softmax function instead of K.softmax\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "#A Capsule Implement with Pure Keras\n",
        "class Capsule(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Capsule, self).build(input_shape)\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(1, input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(input_num_capsule,\n",
        "                                            input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
        "        else:\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(u_vecs)[0]\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
        "                                            self.num_capsule, self.dim_capsule))\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "\n",
        "        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = K.batch_dot(c, u_hat_vecs, [2, 2])\n",
        "            if K.backend() == 'theano':\n",
        "                o = K.sum(o, axis=1)\n",
        "            if i < self.routings - 1:\n",
        "                o = K.l2_normalize(o, -1)\n",
        "                b = K.batch_dot(o, u_hat_vecs, [2, 3])\n",
        "                if K.backend() == 'theano':\n",
        "                    b = K.sum(b, axis=1)\n",
        "\n",
        "        return self.activation(o)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54ToYcOUj34o",
        "outputId": "ee542694-9b02-44b8-c9e4-facab15bfae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 4500\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
        "\n",
        "    x = Bidirectional(GRU(gru_len,\n",
        "                          activation='relu',\n",
        "                          dropout=dropout_p,\n",
        "                          recurrent_dropout=dropout_p,\n",
        "                          return_sequences=True))(embed_layer)\n",
        "    capsule = Capsule(\n",
        "        num_capsule=Num_capsule,\n",
        "        dim_capsule=Dim_capsule,\n",
        "        routings=Routings,\n",
        "        share_weights=True)(x)\n",
        "\n",
        "    capsule = Flatten()(capsule)\n",
        "    capsule = Dropout(dropout_p)(capsule)\n",
        "    capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=input1, outputs=output)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=4500):\n",
        "    # (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=maxlen)\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    return X, y_train, Xt, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train, y_train, x_test, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(x_test, y_test))\n",
        "    model.save(\"/content/drive/My Drive/Rumor.h5\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 4500)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3032: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 4500)              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 4500, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 4500, 256)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 4500, 512)         787968    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2304000)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 2304001   \n",
            "=================================================================\n",
            "Total params: 8,211,969\n",
            "Trainable params: 8,211,969\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 3728s 227ms/step - loss: 0.4066 - acc: 0.8550 - val_loss: 0.1330 - val_acc: 0.9601\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 3644s 222ms/step - loss: 0.0695 - acc: 0.9782 - val_loss: 0.1042 - val_acc: 0.9701\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 3614s 220ms/step - loss: 0.0374 - acc: 0.9888 - val_loss: 0.1243 - val_acc: 0.9681\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 3599s 219ms/step - loss: 0.0324 - acc: 0.9901 - val_loss: 0.1251 - val_acc: 0.9701\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 3590s 218ms/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.1197 - val_acc: 0.9701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r-Y0u2GXXpA",
        "outputId": "b99e64a1-64ca-49f7-aa27-a9da572f3927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers\n",
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
        "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
        "    output: shape=[dim_1, ..., dim_{n-1}]\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
        "    Output shape: [None, d2]\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
        "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of vectors of capsules\n",
        "            x = inputs\n",
        "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
        "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
        "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
        "\n",
        "        # masked inputs, shape = [batch_size, dim_vector]\n",
        "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
        "        return inputs_masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][-1]])\n",
        "        else:\n",
        "            return tuple([None, input_shape[-1]])\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
        "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
        "    :param num_routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_vector = dim_vector\n",
        "        self.num_routing = num_routing\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_vector = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(\n",
        "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
        "            initializer=self.kernel_initializer,\n",
        "            name='W')\n",
        "\n",
        "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
        "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    name='bias',\n",
        "                                    trainable=False)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
        "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
        "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
        "\n",
        "        \"\"\"  \n",
        "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
        "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
        "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
        "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
        "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
        "        \"\"\"\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
        "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
        "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
        "                             elems=inputs_tiled,\n",
        "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
        "        \"\"\"\n",
        "        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n",
        "        def body(i, b, outputs):\n",
        "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
        "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
        "            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
        "            return [i-1, b, outputs]\n",
        "        cond = lambda i, b, inputs_hat: i > 0\n",
        "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
        "        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n",
        "        \"\"\"\n",
        "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
        "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
        "        for i in range(self.num_routing):\n",
        "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
        "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
        "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
        "\n",
        "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
        "            if i != self.num_routing - 1:\n",
        "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
        "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
        "                # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
        "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_vector])\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding, name):\n",
        "    \"\"\"\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_vector: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
        "    \"\"\"\n",
        "    output = layers.Conv1D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding, name=name)(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
        "    return layers.Lambda(squash)(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvI-1U9-YEJj",
        "outputId": "d074f94d-69dc-4aa3-a5ca-2764d88f2db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primary_caps)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=input1, outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    return X, y_train, Xt, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train, y_train, x_test, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(x_test, y_test))\n",
        "    model.save(\"/content/drive/My Drive/RumorCNNLSTM5.h5\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-7-2fbe51974bde>:135: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 1000, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1000, 512)         1050624   \n",
            "_________________________________________________________________\n",
            "conv1 (Conv1D)               (None, 992, 256)          1179904   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 992, 256)          0         \n",
            "_________________________________________________________________\n",
            "primary_caps (Conv1D)        (None, 492, 256)          590080    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 15744, 8)          0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 15744, 8)          0         \n",
            "_________________________________________________________________\n",
            "category_caps (CapsuleLayer) (None, 1, 16)             2030976   \n",
            "_________________________________________________________________\n",
            "out_caps (Length)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 9,971,584\n",
            "Trainable params: 9,955,840\n",
            "Non-trainable params: 15,744\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 976s 59ms/step - loss: 0.3545 - acc: 0.8546 - val_loss: 0.3018 - val_acc: 0.8657\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 962s 59ms/step - loss: 0.1495 - acc: 0.9462 - val_loss: 0.1647 - val_acc: 0.9431\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 957s 58ms/step - loss: 0.1220 - acc: 0.9569 - val_loss: 0.1439 - val_acc: 0.9431\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 956s 58ms/step - loss: 0.0995 - acc: 0.9627 - val_loss: 0.1214 - val_acc: 0.9608\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 959s 58ms/step - loss: 0.0699 - acc: 0.9766 - val_loss: 0.1223 - val_acc: 0.9557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOkyeTEs44i6",
        "outputId": "c0395f75-0e2e-4c5b-90a9-dc99e7646b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    input2 = Input(shape = (maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer2 = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input2)\n",
        "              \n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x1 = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer2)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    primary_caps2 = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps2\")\n",
        "    primaryconcat = concatenate([primary_caps,primary_caps2], axis=-1)\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primaryconcat)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    tokenizer_title = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer_title.fit_on_texts(texts = x_train['title'])\n",
        "    X_title = tokenizer_title.texts_to_sequences(texts = x_train['title'])\n",
        "    X_title = pad_sequences(sequences = X_title, maxlen = maxlen)\n",
        "    Xt_title = tokenizer_title.texts_to_sequences(texts=x_test['title'])\n",
        "    Xt_title = pad_sequences(sequences = Xt_title, maxlen = maxlen)\n",
        "    return X,X_title, y_train, Xt,Xt_title, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train,x_train1, y_train, x_test,x_test1, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit([x_train,x_train1], y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=([x_test,x_test1], y_test))\n",
        "    model.save(\"/content/drive/My Drive/RumorCNNLSTM5new.h5\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1000, 256)    5120000     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 1000, 256)    5120000     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 1000, 512)    1050624     embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 1000, 512)    1050624     embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 1000, 1024)   0           bidirectional_6[0][0]            \n",
            "                                                                 bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 992, 256)     2359552     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 992, 256)     0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps (Conv1D)           (None, 492, 256)     590080      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps2 (Conv1D)          (None, 492, 256)     590080      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 15744, 8)     0           primary_caps[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 15744, 8)     0           primary_caps2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 15744, 8)     0           reshape_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 15744, 8)     0           reshape_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 15744, 16)    0           lambda_6[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "category_caps (CapsuleLayer)    (None, 1, 16)        4046208     concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 1)            0           category_caps[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 19,927,168\n",
            "Trainable params: 19,911,424\n",
            "Non-trainable params: 15,744\n",
            "__________________________________________________________________________________________________\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 1871s 114ms/step - loss: 0.3893 - acc: 0.8546 - val_loss: 0.1862 - val_acc: 0.9448\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 1863s 113ms/step - loss: 0.1631 - acc: 0.9476 - val_loss: 0.1142 - val_acc: 0.9608\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 1857s 113ms/step - loss: 0.0906 - acc: 0.9698 - val_loss: 0.1178 - val_acc: 0.9647\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 1861s 113ms/step - loss: 0.0658 - acc: 0.9779 - val_loss: 0.1205 - val_acc: 0.9681\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 1880s 114ms/step - loss: 0.0566 - acc: 0.9820 - val_loss: 0.1073 - val_acc: 0.9715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "235zj-RqRVXI",
        "outputId": "ac4e2ffc-9a3c-4421-a5a3-f53aa199a29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    input2 = Input(shape = (maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer2 = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input2)\n",
        "              \n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x)\n",
        "    x1 = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer2)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    x1 = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x1)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    x1 = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x1)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    primary_caps2 = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps2\")\n",
        "    primaryconcat = concatenate([primary_caps,primary_caps2], axis=-1)\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primaryconcat)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    tokenizer_title = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer_title.fit_on_texts(texts = x_train['title'])\n",
        "    X_title = tokenizer_title.texts_to_sequences(texts = x_train['title'])\n",
        "    X_title = pad_sequences(sequences = X_title, maxlen = maxlen)\n",
        "    Xt_title = tokenizer_title.texts_to_sequences(texts=x_test['title'])\n",
        "    Xt_title = pad_sequences(sequences = Xt_title, maxlen = maxlen)\n",
        "    return X,X_title, y_train, Xt,Xt_title, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train,x_train1, y_train, x_test,x_test1, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit([x_train,x_train1], y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=([x_test,x_test1], y_test))\n",
        "    # model.save(\"/content/drive/My Drive/RumorCNNLSTM5new.h5\")\n",
        "    result = model.predict([x_test,x_test1])\n",
        "    y_pred = np.argmax(result, axis=-1)\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(y_test,y_pred))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_33 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_34 (InputLayer)           (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_33 (Embedding)        (None, 1000, 256)    5120000     input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_34 (Embedding)        (None, 1000, 256)    5120000     input_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_47 (Bidirectional (None, 1000, 512)    1050624     embedding_33[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_50 (Bidirectional (None, 1000, 512)    1050624     embedding_34[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 1000, 512)    0           bidirectional_47[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 1000, 512)    0           bidirectional_50[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_48 (Bidirectional (None, 1000, 256)    656384      dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_51 (Bidirectional (None, 1000, 256)    656384      dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 1000, 256)    0           bidirectional_48[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 1000, 256)    0           bidirectional_51[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_49 (Bidirectional (None, 1000, 128)    164352      dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_52 (Bidirectional (None, 1000, 128)    164352      dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 1000, 256)    0           bidirectional_49[0][0]           \n",
            "                                                                 bidirectional_52[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 992, 256)     590080      concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 992, 256)     0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps (Conv1D)           (None, 492, 256)     590080      dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps2 (Conv1D)          (None, 492, 256)     590080      dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_45 (Reshape)            (None, 15744, 8)     0           primary_caps[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_46 (Reshape)            (None, 15744, 8)     0           primary_caps2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 15744, 8)     0           reshape_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_46 (Lambda)              (None, 15744, 8)     0           reshape_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 15744, 16)    0           lambda_45[0][0]                  \n",
            "                                                                 lambda_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "category_caps (CapsuleLayer)    (None, 1, 16)        4046208     concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 1)            0           category_caps[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 19,799,168\n",
            "Trainable params: 19,783,424\n",
            "Non-trainable params: 15,744\n",
            "__________________________________________________________________________________________________\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 5607s 341ms/step - loss: 0.4793 - acc: 0.8070 - val_loss: 0.1416 - val_acc: 0.9572\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 5508s 335ms/step - loss: 0.0832 - acc: 0.9703 - val_loss: 0.0676 - val_acc: 0.9745\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 5609s 341ms/step - loss: 0.0584 - acc: 0.9783 - val_loss: 0.0909 - val_acc: 0.9672\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 5531s 336ms/step - loss: 0.3603 - acc: 0.9035 - val_loss: 0.1452 - val_acc: 0.9482\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 5580s 339ms/step - loss: 0.1105 - acc: 0.9594 - val_loss: 0.1384 - val_acc: 0.9475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ-IiXo_DrVU",
        "outputId": "b884139a-83c0-4866-dd48-3da6008312e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    input2 = Input(shape = (maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer2 = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input2)\n",
        "              \n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x)\n",
        "    x1 = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer2)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    x1 = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x1)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    # x1 = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x1)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    primary_caps2 = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps2\")\n",
        "    primaryconcat = concatenate([primary_caps,primary_caps2], axis=-1)\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primaryconcat)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    tokenizer_title = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer_title.fit_on_texts(texts = x_train['title'])\n",
        "    X_title = tokenizer_title.texts_to_sequences(texts = x_train['title'])\n",
        "    X_title = pad_sequences(sequences = X_title, maxlen = maxlen)\n",
        "    Xt_title = tokenizer_title.texts_to_sequences(texts=x_test['title'])\n",
        "    Xt_title = pad_sequences(sequences = Xt_title, maxlen = maxlen)\n",
        "    return X,X_title, y_train, Xt,Xt_title, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train,x_train1, y_train, x_test,x_test1, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit([x_train,x_train1], y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=([x_test,x_test1], y_test))\n",
        "    # model.save(\"/content/drive/My Drive/RumorCNNLSTM5new.h5\")\n",
        "    result = model.predict([x_test,x_test1])\n",
        "    # print(result)\n",
        "    y_pred = []\n",
        "    for i in result:\n",
        "      if i>=0.5:\n",
        "        y_pred.append(1)\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "\n",
        "    # y_pred = np.argmax(result, axis=-1)\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(y_test,y_pred))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 1000, 256)    5120000     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1000, 256)    5120000     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_9 (Bidirectional) (None, 1000, 512)    1050624     embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_11 (Bidirectional (None, 1000, 512)    1050624     embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 1000, 512)    0           bidirectional_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 1000, 512)    0           bidirectional_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_10 (Bidirectional (None, 1000, 256)    656384      dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_12 (Bidirectional (None, 1000, 256)    656384      dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 1000, 256)    0           bidirectional_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 1000, 256)    0           bidirectional_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 1000, 512)    0           dropout_12[0][0]                 \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 992, 256)     1179904     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 992, 256)     0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps (Conv1D)           (None, 492, 256)     590080      dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps2 (Conv1D)          (None, 492, 256)     590080      dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 15744, 8)     0           primary_caps[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 15744, 8)     0           primary_caps2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 15744, 8)     0           reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 15744, 8)     0           reshape_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 15744, 16)    0           lambda_5[0][0]                   \n",
            "                                                                 lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "category_caps (CapsuleLayer)    (None, 1, 16)        4046208     concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 1)            0           category_caps[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 20,060,288\n",
            "Trainable params: 20,044,544\n",
            "Non-trainable params: 15,744\n",
            "__________________________________________________________________________________________________\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 2945s 179ms/step - loss: 0.4777 - acc: 0.8248 - val_loss: 0.4861 - val_acc: 0.8694\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 2933s 178ms/step - loss: 0.2076 - acc: 0.9414 - val_loss: 0.0969 - val_acc: 0.9657\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 2852s 173ms/step - loss: 0.0955 - acc: 0.9668 - val_loss: 0.1211 - val_acc: 0.9582\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 2894s 176ms/step - loss: 0.0849 - acc: 0.9713 - val_loss: 0.0841 - val_acc: 0.9711\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 2925s 178ms/step - loss: 0.0706 - acc: 0.9766 - val_loss: 0.1351 - val_acc: 0.9543\n",
            "[[1955   83]\n",
            " [ 105 1968]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95      2038\n",
            "           1       0.96      0.95      0.95      2073\n",
            "\n",
            "    accuracy                           0.95      4111\n",
            "   macro avg       0.95      0.95      0.95      4111\n",
            "weighted avg       0.95      0.95      0.95      4111\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdYZsBQ_51ut",
        "outputId": "19b90387-01f6-4789-b094-fb30836a7b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    input2 = Input(shape = (maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer2 = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input2)\n",
        "              \n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x)\n",
        "    x1 = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer2)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    x1 = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x1)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    # x1 = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x1)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    primary_caps2 = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps2\")\n",
        "    primaryconcat = concatenate([primary_caps,primary_caps2], axis=-1)\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primaryconcat)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    tokenizer_title = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer_title.fit_on_texts(texts = x_train['title'])\n",
        "    X_title = tokenizer_title.texts_to_sequences(texts = x_train['title'])\n",
        "    X_title = pad_sequences(sequences = X_title, maxlen = maxlen)\n",
        "    Xt_title = tokenizer_title.texts_to_sequences(texts=x_test['title'])\n",
        "    Xt_title = pad_sequences(sequences = Xt_title, maxlen = maxlen)\n",
        "    return X,X_title, y_train, Xt,Xt_title, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train,x_train1, y_train, x_test,x_test1, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit([x_train,x_train1], y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=([x_test,x_test1], y_test))\n",
        "    # model.save(\"/content/drive/My Drive/RumorCNNLSTM5new.h5\")\n",
        "    result = model.predict([x_test,x_test1])\n",
        "    # print(result)\n",
        "    y_pred = []\n",
        "    count1= 0\n",
        "    count2 = 0\n",
        "    count3 = 0\n",
        "    count4 = 0\n",
        "    count5 = 0\n",
        "    for i in result:\n",
        "      if i>=0.5:\n",
        "        y_pred.append(1)\n",
        "        if i <=0.6:\n",
        "          count1+=1\n",
        "        elif i >0.6 and i<=0.7:\n",
        "          count2+=1\n",
        "        elif i>0.7 and i<=0.8:\n",
        "          count3+=1\n",
        "        elif i>0.8 and i<=0.9:\n",
        "          count4+=1\n",
        "        elif i>0.9 and i<=1.0:\n",
        "          count5+=1\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "\n",
        "    print(count1)\n",
        "    print(count2)\n",
        "    print(count3)\n",
        "    print(count4)\n",
        "    print(count5)\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-5-2fbe51974bde>:135: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1000, 256)    5120000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1000, 256)    5120000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 1000, 512)    1050624     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 1000, 512)    1050624     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1000, 512)    0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1000, 512)    0           bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 1000, 256)    656384      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 1000, 256)    656384      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1000, 256)    0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1000, 256)    0           bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1000, 512)    0           dropout_2[0][0]                  \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 992, 256)     1179904     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 992, 256)     0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps (Conv1D)           (None, 492, 256)     590080      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps2 (Conv1D)          (None, 492, 256)     590080      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 15744, 8)     0           primary_caps[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 15744, 8)     0           primary_caps2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 15744, 8)     0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 15744, 8)     0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 15744, 16)    0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "category_caps (CapsuleLayer)    (None, 1, 16)        4046208     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 1)            0           category_caps[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 20,060,288\n",
            "Trainable params: 20,044,544\n",
            "Non-trainable params: 15,744\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 3006s 183ms/step - loss: 0.5422 - acc: 0.7887 - val_loss: 0.0998 - val_acc: 0.9667\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 3036s 185ms/step - loss: 0.1185 - acc: 0.9566 - val_loss: 0.0803 - val_acc: 0.9708\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 2976s 181ms/step - loss: 0.0492 - acc: 0.9825 - val_loss: 0.0636 - val_acc: 0.9766\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 2916s 177ms/step - loss: 0.0326 - acc: 0.9891 - val_loss: 0.0761 - val_acc: 0.9769\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 2981s 181ms/step - loss: 0.0293 - acc: 0.9911 - val_loss: 0.1060 - val_acc: 0.9720\n",
            "4\n",
            "3\n",
            "20\n",
            "31\n",
            "2002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_o2V8UXyEdd",
        "outputId": "43436b1f-3ef0-4720-8eb5-8e7dd2b6590c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    input2 = Input(shape = (maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer2 = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input2)\n",
        "              \n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x)\n",
        "    x1 = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer2)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    x1 = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x1)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    # x1 = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x1)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    primary_caps2 = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps2\")\n",
        "    primaryconcat = concatenate([primary_caps,primary_caps2], axis=-1)\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primaryconcat)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    tokenizer_title = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer_title.fit_on_texts(texts = x_train['title'])\n",
        "    X_title = tokenizer_title.texts_to_sequences(texts = x_train['title'])\n",
        "    X_title = pad_sequences(sequences = X_title, maxlen = maxlen)\n",
        "    Xt_title = tokenizer_title.texts_to_sequences(texts=x_test['title'])\n",
        "    Xt_title = pad_sequences(sequences = Xt_title, maxlen = maxlen)\n",
        "    return X,X_title, y_train, Xt,Xt_title, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train,x_train1, y_train, x_test,x_test1, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit([x_train,x_train1], y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=([x_test,x_test1], y_test))\n",
        "    # model.save(\"/content/drive/My Drive/RumorCNNLSTM5new.h5\")\n",
        "    result = model.predict([x_test,x_test1])\n",
        "    # print(result)\n",
        "    y_pred = []\n",
        "    count1= 0\n",
        "    count2 = 0\n",
        "    count3 = 0\n",
        "    count4 = 0\n",
        "    count5 = 0\n",
        "    for i in result:\n",
        "      if i>=0.5:\n",
        "        y_pred.append(1)\n",
        "        if i <=0.92:\n",
        "          count1+=1\n",
        "        elif i >0.92 and i<=0.94:\n",
        "          count2+=1\n",
        "        elif i>0.94 and i<=0.96:\n",
        "          count3+=1\n",
        "        elif i>0.96 and i<=0.98:\n",
        "          count4+=1\n",
        "        elif i>0.98 and i<=1.0:\n",
        "          count5+=1\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "\n",
        "    print(count1)\n",
        "    print(count2)\n",
        "    print(count3)\n",
        "    print(count4)\n",
        "    print(count5)\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-5-2fbe51974bde>:135: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1000, 256)    5120000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1000, 256)    5120000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 1000, 512)    1050624     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 1000, 512)    1050624     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1000, 512)    0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1000, 512)    0           bidirectional_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 1000, 256)    656384      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 1000, 256)    656384      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1000, 256)    0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1000, 256)    0           bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1000, 512)    0           dropout_2[0][0]                  \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 992, 256)     1179904     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 992, 256)     0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps (Conv1D)           (None, 492, 256)     590080      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps2 (Conv1D)          (None, 492, 256)     590080      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 15744, 8)     0           primary_caps[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 15744, 8)     0           primary_caps2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 15744, 8)     0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 15744, 8)     0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 15744, 16)    0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "category_caps (CapsuleLayer)    (None, 1, 16)        4046208     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 1)            0           category_caps[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 20,060,288\n",
            "Trainable params: 20,044,544\n",
            "Non-trainable params: 15,744\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 3881s 236ms/step - loss: 0.5684 - acc: 0.7875 - val_loss: 0.1713 - val_acc: 0.9438\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 3836s 233ms/step - loss: 0.0857 - acc: 0.9682 - val_loss: 0.0678 - val_acc: 0.9771\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 3765s 229ms/step - loss: 0.0465 - acc: 0.9839 - val_loss: 0.0641 - val_acc: 0.9796\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 3781s 230ms/step - loss: 0.0308 - acc: 0.9892 - val_loss: 0.0734 - val_acc: 0.9774\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 3743s 228ms/step - loss: 0.0192 - acc: 0.9925 - val_loss: 0.0843 - val_acc: 0.9764\n",
            "70\n",
            "14\n",
            "19\n",
            "44\n",
            "1923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Li5t8QuD4Sy",
        "outputId": "8a7b8bf0-f6f8-4d4d-cb83-ca6cfbd36deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from keras.layers import K, Activation\n",
        "# from keras.engine import Layer\n",
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "# from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from vendor.Capsule.Capsule_Keras import *\n",
        "\n",
        "gru_len = 256\n",
        "Routings = 3\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size = 256\n",
        "\n",
        "def get_model():\n",
        "    input1 = Input(shape=(maxlen,))\n",
        "    input2 = Input(shape = (maxlen,))\n",
        "    embed_layer = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input1)\n",
        "    embed_layer2 = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen)(input2)\n",
        "              \n",
        "    x = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x)\n",
        "    x1 = Bidirectional(LSTM(gru_len, return_sequences=True))(embed_layer2)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    x1 = Bidirectional(LSTM(int(gru_len/2), return_sequences=True))(x1)\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    # x1 = Bidirectional(LSTM(int(gru_len/4), return_sequences=True))(x1)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    # x = SpatialDropout1D(rate_drop_dense)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    primary_caps2 = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps2\")\n",
        "    primaryconcat = concatenate([primary_caps,primary_caps2], axis=-1)\n",
        "    category_caps = CapsuleLayer(num_capsule=1, dim_vector=16, num_routing=3, name='category_caps')(primaryconcat)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    # # x = Bidirectional(GRU(gru_len,\n",
        "    # #                       activation='relu',\n",
        "    # #                       dropout=dropout_p,\n",
        "    # #                       recurrent_dropout=dropout_p,\n",
        "    # #                       return_sequences=True))(embed_layer)\n",
        "    # x = Conv1D(filters=512, kernel_size=4, padding=\"valid\")(x)\n",
        "    # x = Dropout(0.7)(x)\n",
        "    # x = Conv1D(filters=256, kernel_size=4, padding=\"valid\")(x)\n",
        "    # # x = Bidirectional(LSTM(gru_len, return_sequences=False))(x)\n",
        "    # capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,activation='relu',\n",
        "    #                   share_weights=True)(x)\n",
        "    # # capsule1 = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule1, routings=Routings,activation='relu',\n",
        "    # #                   share_weights=True)(x)\n",
        "\n",
        "    # # capsule = concatenate([capsule, capsule1], axis=-1)\n",
        "\n",
        "    # capsule = Flatten()(capsule)\n",
        "    # capsule = Dropout(dropout_p)(capsule)\n",
        "    \n",
        "    # # capsule = LeakyReLU()(capsule)\n",
        "\n",
        "    # # output = Dense(1, activation='sigmoid')(x)\n",
        "    # output = Dense(1, activation='sigmoid')(out_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_imdb(maxlen=1000):\n",
        "    x_train = pd.read_csv(\"/content/drive/My Drive/Rumortrain.csv\",encoding='latin1')\n",
        "    x_test = pd.read_csv(\"/content/drive/My Drive/Rumortest.csv\",encoding='latin1')\n",
        "    y_train = x_train['label'].values\n",
        "    y_test = x_test['label'].values\n",
        "    tokenizer = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer.fit_on_texts(texts = x_train['text'])\n",
        "    X = tokenizer.texts_to_sequences(texts = x_train['text'])\n",
        "    X = pad_sequences(sequences = X, maxlen = maxlen)\n",
        "    print(X.shape)\n",
        "    Xt = tokenizer.texts_to_sequences(texts=x_test['text'])\n",
        "    Xt = pad_sequences(sequences = Xt, maxlen = maxlen)\n",
        "    # x_train = sequence.pad_sequences(x_train['text'], maxlen=maxlen)\n",
        "    # x_test = sequence.pad_sequences(x_test['text'], maxlen=maxlen)\n",
        "    tokenizer_title = Tokenizer(num_words = maxlen, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower = True, split = ' ')\n",
        "    tokenizer_title.fit_on_texts(texts = x_train['title'])\n",
        "    X_title = tokenizer_title.texts_to_sequences(texts = x_train['title'])\n",
        "    X_title = pad_sequences(sequences = X_title, maxlen = maxlen)\n",
        "    Xt_title = tokenizer_title.texts_to_sequences(texts=x_test['title'])\n",
        "    Xt_title = pad_sequences(sequences = Xt_title, maxlen = maxlen)\n",
        "    return X,X_title, y_train, Xt,Xt_title, y_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    x_train,x_train1, y_train, x_test,x_test1, y_test = load_imdb()\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    batch_size = 64\n",
        "    epochs = 5\n",
        "\n",
        "    model.fit([x_train,x_train1], y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=([x_test,x_test1], y_test))\n",
        "    # model.save(\"/content/drive/My Drive/RumorCNNLSTM5new.h5\")\n",
        "    result = model.predict([x_test,x_test1])\n",
        "    print(result)\n",
        "    result = np.array(result)\n",
        "    np.savetxt('/content/drive/My Drive/Rumoroutput.txt', result, delimiter='\\n') \n",
        "    # with open(\"/content/drive/My Drive/Rumoroutput.txt\", \"w\") as txt_file:\n",
        "    #   for line in result:\n",
        "    #     txt_file.write(line)\n",
        "    #     txt_file.write(\"\\n\")\n",
        "    # y_pred = []\n",
        "    # count1= 0\n",
        "    # count2 = 0\n",
        "    # count3 = 0\n",
        "    # count4 = 0\n",
        "    # count5 = 0\n",
        "    # for i in result:\n",
        "    #   if i>=0.5:\n",
        "    #     y_pred.append(1)\n",
        "    #     if i <=0.92:\n",
        "    #       count1+=1\n",
        "    #     elif i >0.92 and i<=0.94:\n",
        "    #       count2+=1\n",
        "    #     elif i>0.94 and i<=0.96:\n",
        "    #       count3+=1\n",
        "    #     elif i>0.96 and i<=0.98:\n",
        "    #       count4+=1\n",
        "    #     elif i>0.98 and i<=1.0:\n",
        "    #       count5+=1\n",
        "    #   else:\n",
        "    #     y_pred.append(0)\n",
        "\n",
        "    # print(count1)\n",
        "    # print(count2)\n",
        "    # print(count3)\n",
        "    # print(count4)\n",
        "    # print(count5)\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16443, 1000)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1000, 256)    5120000     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1000, 256)    5120000     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 1000, 512)    1050624     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 1000, 512)    1050624     embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 1000, 512)    0           bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 1000, 512)    0           bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 1000, 256)    656384      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_8 (Bidirectional) (None, 1000, 256)    656384      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 1000, 256)    0           bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 1000, 256)    0           bidirectional_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1000, 512)    0           dropout_7[0][0]                  \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv1D)                  (None, 992, 256)     1179904     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 992, 256)     0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps (Conv1D)           (None, 492, 256)     590080      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "primary_caps2 (Conv1D)          (None, 492, 256)     590080      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 15744, 8)     0           primary_caps[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 15744, 8)     0           primary_caps2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 15744, 8)     0           reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 15744, 8)     0           reshape_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 15744, 16)    0           lambda_3[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "category_caps (CapsuleLayer)    (None, 1, 16)        4046208     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_caps (Length)               (None, 1)            0           category_caps[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 20,060,288\n",
            "Trainable params: 20,044,544\n",
            "Non-trainable params: 15,744\n",
            "__________________________________________________________________________________________________\n",
            "Train on 16443 samples, validate on 4111 samples\n",
            "Epoch 1/5\n",
            "16443/16443 [==============================] - 2887s 176ms/step - loss: 0.4248 - acc: 0.8250 - val_loss: 0.2320 - val_acc: 0.9207\n",
            "Epoch 2/5\n",
            "16443/16443 [==============================] - 2861s 174ms/step - loss: 0.1177 - acc: 0.9569 - val_loss: 0.1360 - val_acc: 0.9543\n",
            "Epoch 3/5\n",
            "16443/16443 [==============================] - 2836s 172ms/step - loss: 0.0941 - acc: 0.9693 - val_loss: 0.0895 - val_acc: 0.9684\n",
            "Epoch 4/5\n",
            "16443/16443 [==============================] - 2841s 173ms/step - loss: 0.0565 - acc: 0.9811 - val_loss: 0.0929 - val_acc: 0.9667\n",
            "Epoch 5/5\n",
            "16443/16443 [==============================] - 2777s 169ms/step - loss: 0.0487 - acc: 0.9839 - val_loss: 0.0718 - val_acc: 0.9740\n",
            "[[0.00137107]\n",
            " [0.00137107]\n",
            " [0.99971795]\n",
            " ...\n",
            " [0.9702403 ]\n",
            " [0.00137107]\n",
            " [0.999055  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}